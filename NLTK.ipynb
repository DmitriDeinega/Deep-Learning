{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "741d4bef-f6cd-4ba6-9bca-b02392f2d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee4cf144-ac6a-4300-9ba4-03a6c2e24a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where',\n",
       " 'is',\n",
       " 'St.',\n",
       " 'Paul',\n",
       " 'located',\n",
       " '?',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'seem',\n",
       " 'to',\n",
       " 'find',\n",
       " 'it',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'in',\n",
       " 'my',\n",
       " 'map',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text = \"Where is St. Paul located? I don't seem to find it. It isn't in my map.\"\n",
    "\n",
    "word_tokenize(my_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ebc69cc-bcd0-44a5-9825-41ee9ad5e047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whoever', 'eat', 'mani', 'cooki', 'is', 'regret', 'do', 'so']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "my_text2 = \"Whoever eats many cookies is regretting doing so\"\n",
    "\n",
    "stemmed_sentence = []\n",
    "\n",
    "for word in word_tokenize(my_text2):\n",
    "    stemmed_sentence.append(ps.stem(word))\n",
    "\n",
    "stemmed_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6cfebf1-ec9b-4970-bc84-9be2003566bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Whoever', 'NNP'),\n",
       " ('eats', 'VBZ'),\n",
       " ('many', 'JJ'),\n",
       " ('cookies', 'NNS'),\n",
       " ('is', 'VBZ'),\n",
       " ('regretting', 'VBG'),\n",
       " ('doing', 'VBG'),\n",
       " ('so', 'RB')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(my_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f532607b-68f8-428f-a8b0-fe0a9fdcd3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whoever', 'eat', 'many', 'cooky', 'be', 'regret', 'do', 'so']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn2wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return wn.NOUN\n",
    "\n",
    "lzr = WordNetLemmatizer()\n",
    "my_text3 = \"Whoever eats many cookies is regretting doing so\"\n",
    "lemed = []\n",
    "for (word, pos) in nltk.pos_tag(word_tokenize(my_text3)):\n",
    "    lemed.append(lzr.lemmatize(word, penn2wn(pos)))\n",
    "\n",
    "lemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd9042b-5d65-43c9-9781-1b1ff86ed25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT big/JJ red/JJ cow/NN)\n",
      "  jumped/VBD\n",
      "  over/IN\n",
      "  (NP the/DT bright/JJ moon/NN))\n",
      "(S\n",
      "  (NounList Dogs/NNS or/CC cats/NNS)\n",
      "  (NounList\n",
      "    Sara/NNP\n",
      "    ,/,\n",
      "    John/NNP\n",
      "    ,/,\n",
      "    Tom/NNP\n",
      "    ,/,\n",
      "    the/DT\n",
      "    girl/NN\n",
      "    and/CC\n",
      "    the/DT\n",
      "    bat/NN))\n",
      "(S\n",
      "  (NounList (NP Dogs/NNS) or/CC (NP cats/NNS))\n",
      "  (NounList\n",
      "    (NP Sara/NNP)\n",
      "    ,/,\n",
      "    (NP John/NNP)\n",
      "    ,/,\n",
      "    (NP Tom/NNP)\n",
      "    ,/,\n",
      "    (NP the/DT girl/NN)\n",
      "    and/CC\n",
      "    (NP the/DT bat/NN)))\n"
     ]
    }
   ],
   "source": [
    "my_text4 = \"the big red cow jumped over the bright moon\"\n",
    "tagged = nltk.pos_tag(word_tokenize(my_text4))\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)\n",
    "# result.draw()\n",
    "\n",
    "my_text5 = \"Dogs or cats Sara, John, Tom, the girl and the bat\"\n",
    "tagged = nltk.pos_tag(word_tokenize(my_text5))\n",
    "grammar = \"NounList: {(<DT>?<NN.?><,>?)+<CC><DT>?<NN.?>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged) \n",
    "print(result)\n",
    "# result.draw()\n",
    "\n",
    "grammar = \"\"\"NP: {<DT>?<JJ>*<NN.?>}\n",
    "            NounList: {(<NP><,>?)+<CC><NP>}\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged) \n",
    "print(result)\n",
    "# result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c94fc9a-9d2e-4c02-9584-017a9a00980c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,504.0,168.0\" width=\"504px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"12.6984%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Bill</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.34921%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"14.2857%\" x=\"12.6984%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Clinton</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.8413%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.93651%\" x=\"26.9841%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.9524%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.93651%\" x=\"34.9206%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"38.8889%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"17.4603%\" x=\"42.8571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">president</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.5873%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.34921%\" x=\"60.3175%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.4921%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.93651%\" x=\"66.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.6349%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"25.3968%\" x=\"74.6032%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"50%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">United</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"50%\" x=\"50%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">States</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNPS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.3016%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Bill', 'NNP')]), Tree('PERSON', [('Clinton', 'NNP')]), ('is', 'VBZ'), ('the', 'DT'), ('president', 'NN'), ('of', 'IN'), ('the', 'DT'), Tree('GPE', [('United', 'NNP'), ('States', 'NNPS')])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text6 = \"Bill Clinton is the president of the United States\"\n",
    "tagged = nltk.pos_tag(word_tokenize(my_text6))\n",
    "result = nltk.ne_chunk(tagged)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b85c49-bb63-4f27-81cb-ff72b36519d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'is', 'a'),\n",
       " ('is', 'a', 'simple'),\n",
       " ('a', 'simple', 'text'),\n",
       " ('simple', 'text', 'this'),\n",
       " ('text', 'this', ','),\n",
       " ('this', ',', 'this'),\n",
       " (',', 'this', 'is'),\n",
       " ('this', 'is', 'a'),\n",
       " ('is', 'a', 'simple'),\n",
       " ('a', 'simple', 'text'),\n",
       " ('simple', 'text', ','),\n",
       " ('text', ',', 'is'),\n",
       " (',', 'is', 'it'),\n",
       " ('is', 'it', 'simple'),\n",
       " ('it', 'simple', '?')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text7 = \"It is a simple text this, this is a simple text, is it simple?\"\n",
    "result = list(nltk.ngrams(word_tokenize(my_text7), 3))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95352a9b-68c9-44e1-8167-0ed955220e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is impossible . `` How many a nature spurts up suddenly like dirty water , when any one should not be preferable ? For there are `` gay science , who sees nothing in philosophy , whatever I have not succeeded in explaining our entire instinctive life in view . Perhaps severity and harshness , constraint , the world without in the internal economy of life for example society , every hand pressure , every step he takes into the form of questions from the gloomy , agreeable nooks in which religious life flourished most vigorously , believe that anybody ever\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from random import choice\n",
    "\n",
    "paragraph_len = 100\n",
    "all_text = urllib.request.urlopen(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\").read().decode(\"utf-8\")\n",
    "tokens = word_tokenize(all_text)\n",
    "my_grams = list(nltk.ngrams(tokens, 3))\n",
    "sentence = [\"It\", \"is\"]\n",
    "\n",
    "for i in range(paragraph_len):\n",
    "    options = []\n",
    "    for trig in my_grams:\n",
    "        if trig[0].lower() == sentence[-2].lower() and trig[1].lower() == sentence[-1].lower():\n",
    "            options.append(trig[2])\n",
    "    if len(options) > 0:\n",
    "        sentence.append(choice(options))\n",
    "\n",
    "print(\" \".join(sentence)) # better detokenize instead of join by \" \". as ',' will have space before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be8fb685-584e-470f-b81b-55d916f07d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N dog) (PP (P with) (NP (Det my) (N telescope))))))\n",
      "(S\n",
      "  (NP Mary)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N dog))\n",
      "    (PP (P with) (NP (Det my) (N telescope)))))\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP | V NP PP\n",
    "PP -> P NP\n",
    "V -> \"saw\" | \"ate\" | \"walked\"\n",
    "NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "P -> \"in\" | \"on\" | \"by\" | \"with\" \"\"\")\n",
    "\n",
    "sentence = \"Mary saw Bob\".split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar)\n",
    "# print(list(rd_parser.parse(sentence))[0])\n",
    "parser = list(rd_parser.parse(\"Mary saw a dog with my telescope\".split()))\n",
    "for parse in parser:\n",
    "    print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e1c0071-17a9-4dcb-afdf-23391e270c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP they)\n",
      "  (VP\n",
      "    (VBP ate)\n",
      "    (NP spaghetti)\n",
      "    (PP (IN with) (NP meatballs)))) (p=0.006)\n"
     ]
    }
   ],
   "source": [
    "'''grammar = nltk.PCFG.fromstring(\"\"\"\n",
    "S -> NP VP [1.0]\n",
    "VP -> TV NP [0.4]\n",
    "VP -> IV [0.3]\n",
    "VP -> DatV NP NP [0.3]\n",
    "TV -> 'saw' [1.0]\n",
    "IV -> 'ate' [1.0]\n",
    "DatV -> 'gave' [1.0]\n",
    "NP -> 'telescopes' [0.8]\n",
    "NP -> 'Jack' [0.2] \"\"\")\n",
    "\n",
    "viterbi_parser = nltk.ViterbiParser(grammar)\n",
    "\n",
    "for tree in viterbi_parser.parse(['Jack', 'saw', 'telescopes']):\n",
    "    print(tree)\n",
    "print('--------')'''\n",
    "grammar = nltk.PCFG.fromstring(\"\"\"\n",
    "S -> NP VP\t\t\t[1.0]\n",
    "VP -> VBP NP\t\t[0.5]\n",
    "VP -> VBP NP PP\t\t[0.5]\n",
    "TV -> 'saw'\t\t\t[1.0]\n",
    "VBP -> 'ate'\t\t[1.0]\n",
    "NP -> NP PP\t\t\t[0.3]\n",
    "PP -> IN NP\t\t\t[1.0]\n",
    "NP -> 'spaghetti'\t[0.2]\n",
    "NP -> 'they' \t\t[0.3]\n",
    "NP -> 'meatballs'\t[0.2]\n",
    "IN -> 'with'\t\t[1.0]\n",
    "\"\"\")\n",
    "\n",
    "viterbi_parser = nltk.ViterbiParser(grammar)\n",
    "\n",
    "for tree in viterbi_parser.parse_all(['they', 'ate', 'spaghetti', 'with', 'meatballs']):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3720b0fa-014c-4682-99a0-a503a5f1ad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}\n",
      "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}\n",
      "{'neg': 0.318, 'neu': 0.536, 'pos': 0.146, 'compound': -0.5023}\n"
     ]
    }
   ],
   "source": [
    "sna = SentimentIntensityAnalyzer()\n",
    "print(sna.polarity_scores(\"The movie was great!\"))\n",
    "print(sna.polarity_scores(\"I liked the book, especially the ending.\"))\n",
    "print(sna.polarity_scores(\"The staff were nice, but the food was terrible.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
